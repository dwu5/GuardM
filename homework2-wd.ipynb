{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PA ST 808: Convolutional Neural Network\n## Homework 2: Convolutional Neural Network\n\n**GEC**<br/>\n**Instructor**: Pavlos Protopapas<br />\n**Maximum Score**: 100\n\n<hr style=\"height:2.4pt\">","metadata":{"id":"qF_K7LSiXKXo"}},{"cell_type":"markdown","source":"### INSTRUCTIONS\n\n\n- This homework is a jupyter notebook. Download and work on it on your local machine.\n\n- This homework should be submitted in pairs.\n\n- Ensure you and your partner together have submitted the homework only once. Multiple submissions of the same work will be penalised and will cost you 2 points.\n\n- Please restart the kernel and run the entire notebook again before you submit.\n\n- Running cells out of order is a common pitfall in Jupyter Notebooks. To make sure your code works restart the kernel and run the whole notebook again before you submit. \n\n- To submit the homework, either one of you upload the working notebook on edStem and click the submit button on the bottom right corner.\n\n- Submit the homework well before the given deadline. Submissions after the deadline will not be graded.\n\n- We have tried to include all the libraries you may need to do the assignment in the imports statement at the top of this notebook. We strongly suggest that you use those and not others as we may not be familiar with them.\n\n- Comment your code well. This would help the graders in case there is any issue with the notebook while running. It is important to remember that the graders will not troubleshoot your code. \n\n- Please use .head() when viewing data. Do not submit a notebook that is **excessively long**. \n\n- In questions that require code to answer, such as \"calculate the $R^2$\", do not just output the value from a cell. Write a `print()` function that includes a reference to the calculated value, **not hardcoded**. For example: \n```\nprint(f'The R^2 is {R:.4f}')\n```\n- Your plots should include clear labels for the $x$ and $y$ axes as well as a descriptive title (\"MSE plot\" is not a descriptive title; \"95 % confidence interval of coefficients of polynomial degree 5\" is).\n\n- **Ensure you make appropraite plots for all the questions it is applicable to, regardless of it being explicitly asked for.**\n\n<hr style=\"height:2pt\">","metadata":{"id":"U3d9Jz9GXKXt"}},{"cell_type":"markdown","source":"### Names of the people who worked on this homework together\n#### /name here/","metadata":{"id":"1Ra4FIpGXKXv"}},{"cell_type":"code","source":"#RUN THIS CELL\nimport os\nimport pathlib\nworking_dir = pathlib.Path().absolute()\n\n# Uncomment the line below to help debug if the path to included images don't show\n#print(working_dir)\nos.chdir(working_dir)","metadata":{"id":"4RG-xoubXKXw","execution":{"iopub.status.busy":"2022-03-23T13:08:15.905530Z","iopub.execute_input":"2022-03-23T13:08:15.906142Z","iopub.status.idle":"2022-03-23T13:08:15.933453Z","shell.execute_reply.started":"2022-03-23T13:08:15.906049Z","shell.execute_reply":"2022-03-23T13:08:15.932766Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Please download the 2.1.0 version of tensorflow for this homework and also tf_keras_vis\n!pip install tf_keras_vis \n# !pip -qq install tensorflow==2.1.0","metadata":{"id":"HEn7U8LdXKXx","outputId":"f89068ee-9d3e-43df-8c6b-04570880a691","execution":{"iopub.status.busy":"2022-03-23T13:08:15.948749Z","iopub.execute_input":"2022-03-23T13:08:15.948972Z","iopub.status.idle":"2022-03-23T13:08:25.461135Z","shell.execute_reply.started":"2022-03-23T13:08:15.948946Z","shell.execute_reply":"2022-03-23T13:08:25.460281Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport requests\nimport zipfile\nimport shutil\nimport json\nimport time\nimport sys\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nimport subprocess\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport tensorflow as tf\n","metadata":{"id":"cU-jH1kbXKXy","execution":{"iopub.status.busy":"2022-03-23T13:08:25.463362Z","iopub.execute_input":"2022-03-23T13:08:25.463652Z","iopub.status.idle":"2022-03-23T13:08:30.086617Z","shell.execute_reply.started":"2022-03-23T13:08:25.463614Z","shell.execute_reply":"2022-03-23T13:08:30.085820Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"tf.__version__","metadata":{"id":"ceixFVRDXKXz","outputId":"a864fb0e-dbb0-4db7-e942-8b921446efca","execution":{"iopub.status.busy":"2022-03-23T13:08:30.087915Z","iopub.execute_input":"2022-03-23T13:08:30.088185Z","iopub.status.idle":"2022-03-23T13:08:30.097758Z","shell.execute_reply.started":"2022-03-23T13:08:30.088148Z","shell.execute_reply":"2022-03-23T13:08:30.096893Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Enable/Disable Eager Execution\n# Reference: https://www.tensorflow.org/guide/eager\n# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, \n# without building graphs\n\n#tf.compat.v1.disable_eager_execution()\n#tf.compat.v1.enable_eager_execution()\n\nprint(\"tensorflow version\", tf.__version__)\nprint(\"keras version\", tf.keras.__version__)\nprint(\"Eager Execution Enabled:\", tf.executing_eagerly())\n\n# Get the number of replicas \nstrategy = tf.distribute.MirroredStrategy()\nprint(\"Number of replicas:\", strategy.num_replicas_in_sync)\n\ndevices = tf.config.experimental.get_visible_devices()\nprint(\"Devices:\", devices)\nprint(tf.config.experimental.list_logical_devices('GPU'))\n\nprint(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\nprint(\"All Pysical Devices\", tf.config.list_physical_devices())\n\n# Better performance with the tf.data API\n# Reference: https://www.tensorflow.org/guide/datac_performance\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntf.random.set_seed(2266)","metadata":{"id":"S3ulEzp2XKX0","outputId":"3e4b8f3c-4a3e-4a7a-9407-b0bfa61eab81","execution":{"iopub.status.busy":"2022-03-23T13:08:30.099496Z","iopub.execute_input":"2022-03-23T13:08:30.100039Z","iopub.status.idle":"2022-03-23T13:08:33.512244Z","shell.execute_reply.started":"2022-03-23T13:08:30.099999Z","shell.execute_reply":"2022-03-23T13:08:33.511540Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Restart runtime and run the below cell to check if you have the correct version\n\nprint(tf.__version__)","metadata":{"id":"3-KrZnE9XKX1","outputId":"82235a61-b54a-4832-953a-cea9aae68ddd","execution":{"iopub.status.busy":"2022-03-23T13:08:33.514475Z","iopub.execute_input":"2022-03-23T13:08:33.514753Z","iopub.status.idle":"2022-03-23T13:08:33.524847Z","shell.execute_reply.started":"2022-03-23T13:08:33.514717Z","shell.execute_reply":"2022-03-23T13:08:33.523915Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow_addons==0.9.1","metadata":{"id":"CbZKAhMKYSrs","outputId":"df418e35-4e20-4d5d-c3bc-6fb00409f010","execution":{"iopub.status.busy":"2022-03-23T13:08:33.525904Z","iopub.execute_input":"2022-03-23T13:08:33.526639Z","iopub.status.idle":"2022-03-23T13:08:51.042696Z","shell.execute_reply.started":"2022-03-23T13:08:33.526599Z","shell.execute_reply":"2022-03-23T13:08:51.041733Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_addons as tfa\nimport numpy as np\nimport os\n\n\n\nimport certifi\nimport urllib3  # For handling https certificate verification \nimport scipy.ndimage as ndimage\nimport tensorflow_datasets as tfds\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"id":"zNZ10-uMXKX2","execution":{"iopub.status.busy":"2022-03-23T13:08:51.044338Z","iopub.execute_input":"2022-03-23T13:08:51.044627Z","iopub.status.idle":"2022-03-23T13:08:52.003279Z","shell.execute_reply.started":"2022-03-23T13:08:51.044591Z","shell.execute_reply":"2022-03-23T13:08:52.002550Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nfrom PIL import Image\n\nfrom matplotlib import pyplot\nimport matplotlib.pylab as plt \nfrom scipy.signal import convolve2d\n%matplotlib inline\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.optimizers import Adam, SGD\n\n#Some imports for getting the CIFAR-10 dataset and for help with visualization*]\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\nfrom tf_keras_vis.saliency import Saliency\nfrom tf_keras_vis.utils import normalize\nfrom matplotlib import cm\nfrom tf_keras_vis.gradcam import Gradcam\n\nimport tensorflow_addons as tfa\n\nimport os\nimport certifi\nimport urllib3  # For handling https certificate verification \nimport scipy.ndimage as ndimage\nimport tensorflow_datasets as tfds\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n## Please download the packages that are missing in your colab environment\n","metadata":{"id":"_M9UT1qdXKX3","execution":{"iopub.status.busy":"2022-03-23T13:08:52.004636Z","iopub.execute_input":"2022-03-23T13:08:52.004866Z","iopub.status.idle":"2022-03-23T13:08:52.567449Z","shell.execute_reply.started":"2022-03-23T13:08:52.004835Z","shell.execute_reply":"2022-03-23T13:08:52.566469Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import BatchNormalization","metadata":{"id":"Sl_NkfnV_Oet","execution":{"iopub.status.busy":"2022-03-23T13:08:52.571792Z","iopub.execute_input":"2022-03-23T13:08:52.572100Z","iopub.status.idle":"2022-03-23T13:08:52.580256Z","shell.execute_reply.started":"2022-03-23T13:08:52.572048Z","shell.execute_reply":"2022-03-23T13:08:52.579588Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#EFF8D0\">\n<h1> Overview </h1> \n\n<br />\n\nIn this homework, we will explore Convolutional Neural Networks (CNNs).  We will begin by building a CNN to classify CIFAR-10 images, a standard pedagogical problem, and use saliency maps to understand what the network is paying attention to. We will then see that CNNs aren't just for classifying. They can serve as image input processing for a variety of tasks, as we will show by training a network to rotate faces upright.\n\n\n<h2> Part 1: Building a Basic CNN Model [50pts total] </h2>\n<br />\n\nIn this question, you will use Keras to create a convolutional neural network for predicting the type of object shown in images from the [CIFAR-10](https://keras.io/datasets/#cifar10-small-image-classification) dataset, which contains 50,000 32x32 training images and 10,000 test images of the same size, with a total of 10 classes.\n\n<br /><br />\n\n<h4> Loading CIFAR-10 and Constructing the Model. </h4>\n<br />\n\nLoad CIFAR-10 and use a combination of the following layers: Conv2D, MaxPooling2D, Dense, Dropout and Flatten Layers (not necessarily in this order, and you can use as many layers as you'd like) to build your classification model. You may use an existing architecture like AlexNet or VGG16, or create one of your own design. However, you should construct the network yourself and not use a pre-written implementation. At least one of your Conv2D layers should have at least 9 filters to be able to do question 1.3.\n<br /><br />\n\nConvolutional neural networks are computationally intensive. We highly recommend that you train your model on a system using GPUs (take a look at Google Colab's runtime settings for accessing a GPU environment free of cost). On CPUs, this training can take over an hour. On GPUs, it can be done within minutes. If you become frustrated having to rerun your model every time you open your notebook, take a look at how to save your model weights as explicitly detailed in **Part 2**, where it is required to save your weights.\n<br /><br />\n\nYou can approach the problems in this question by first creating a model assigning 32 filters to each Conv2D layer recreate the model with 64 filters/layer, 128, etc. For each generated model, keep track of the total number of parameters.\n<br /><br />\n\n**1.1** [16pts] Report the total number of parameters in your model. How does the number of total parameters change (linearly, exponentially) as the number of filters per layer increases (your model should have at least 2 Conv layers)? You can find this empirically by constructing multiple models with the same type of architecture, increasing the number of filters. Generate a plot showing the relationship and explain why it has this relationship?\n<br /><br />\n\n**1.2** [20pts total] Choose a model, train and evaluate it.\n<br /><br />\n\n **1.2.1** [15pts] Take your model from above and train it. You can choose to train your model for as long as you'd like, but you should aim for at least 10 epochs.  Your validation accuracy should exceed 70%. Training for 10 epochs on a CPU should take about 30-60 minutes. \n <br /><br />\n \n**1.2.2** [5pts] Plot the loss and accuracy (both train and test) for your chosen architecture.\n <br /><br />\n \n*Techniques to Visualize the Model.*\n <br /><br />\n \nWe will gain an intuition into how our model is processing the inputs in two ways.  First we'll ask you to use feature maps to visualize the activations in the intermediate layers of the network. We've provided a helper function `get_feature_maps` to aid in extracting feature maps from layer outputs in your model network.  Feel free to take advantage of it if you'd like.  We'll also ask you to use [saliency maps](https://arxiv.org/abs/1312.6034) to visualize the pixels that have the largest impact on the classification of an input (image in this case), as well as a more recent development,[Grad-CAM](https://arxiv.org/abs/1610.02391), which has been shown to better indicate the attention of CNNs.\n <br /><br />\n \n**1.3** [14pts] For a given input image from the test set that is correctly classified, use your model and extract 9 feature maps from any intermediate convolutional layer of your choice and plot the images in a 3x3 grid (use `imshow`'s `cmap='gray'` to show the feature maps in black & white).  Make sure to plot (and clearly label) your original input image as well. You may use the provided `get_feature_maps` function and the `cifar10dict` dictionary to convert class index to the correct class name.\n <br />\n</div>\n\n\n","metadata":{"id":"n_o2cdolXKX3"}},{"cell_type":"markdown","source":"### 1.1\n\n\n**1.1 [16pts]** Report the total number of parameters in your model. How does the number of total parameters change (linearly, exponentially) as the number of filters per layer increases (your model should have at least 2 Conv layers)? You can find this empirically by constructing multiple models with the same type of architecture, increasing the number of filters. Generate a plot showing the relationship and explain why it has this relationship.","metadata":{"id":"7udZM-Z8XKX4"}},{"cell_type":"code","source":"# your code here\n\n# load CIFAR-10\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()","metadata":{"id":"dmliGT9xXKX5","outputId":"4ce102f5-f951-4a0f-e4aa-0250ee34841d","execution":{"iopub.status.busy":"2022-03-23T13:08:52.583446Z","iopub.execute_input":"2022-03-23T13:08:52.585777Z","iopub.status.idle":"2022-03-23T13:08:58.241781Z","shell.execute_reply.started":"2022-03-23T13:08:52.583672Z","shell.execute_reply":"2022-03-23T13:08:58.241041Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# check the data shape\nprint('X_train shape: ', X_train.shape)\nprint('y_train shape: ', y_train.shape) \nprint('X_test shape: ', X_test.shape)\nprint('y_test shape: ', y_test.shape) ","metadata":{"id":"eomjw8M1lI6q","outputId":"8ed699bc-7997-45aa-91fc-88c74bf0e84b","execution":{"iopub.status.busy":"2022-03-23T13:08:58.243276Z","iopub.execute_input":"2022-03-23T13:08:58.243540Z","iopub.status.idle":"2022-03-23T13:08:58.251936Z","shell.execute_reply.started":"2022-03-23T13:08:58.243504Z","shell.execute_reply":"2022-03-23T13:08:58.251085Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# build the basic model\ndef build_basic(filters_1, filters_2, filters_3):\n  model = Sequential()\n\n  # get the feature\n  model.add(Conv2D(filters_1, kernel_size=3, strides=1, padding='same', input_shape=X_train.shape[1:]))\n  model.add(Activation('relu'))\n  model.add(Conv2D(filters_1, kernel_size=3))\n  model.add(Activation('relu'))\n  model.add(MaxPooling2D(pool_size=(2, 2)))\n  model.add(Conv2D(filters_2, kernel_size=3, strides=1, padding='same'))\n  model.add(Activation('relu'))\n  model.add(Conv2D(filters_2, kernel_size=3, strides=1))\n  model.add(Activation('relu'))\n  model.add(MaxPooling2D(pool_size=(2, 2)))\n  model.add(Conv2D(filters_3, kernel_size=3, strides=1, padding='same'))\n  model.add(Activation('relu'))\n  model.add(Conv2D(filters_3, kernel_size=3, strides=1))\n  model.add(Activation('relu'))\n  model.add(MaxPooling2D(pool_size=(2, 2)))\n\n  # classify\n  model.add(Flatten())\n  model.add(Dense(256))\n  model.add(Activation('relu'))\n  model.add(Dropout(0.3))\n  model.add(Dense(10))\n  model.add(Activation('softmax'))\n\n  return model","metadata":{"id":"4aiaFvRJlQ9A","execution":{"iopub.status.busy":"2022-03-23T13:08:58.253402Z","iopub.execute_input":"2022-03-23T13:08:58.253841Z","iopub.status.idle":"2022-03-23T13:08:58.265712Z","shell.execute_reply.started":"2022-03-23T13:08:58.253806Z","shell.execute_reply":"2022-03-23T13:08:58.264907Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# build a model\nfilters1_1 = 32\nfilters1_2 = 64\nfilters1_3 = 128\nmodel_1 = build_basic(filters1_1, filters1_2, filters1_3)","metadata":{"id":"m8dCsu4VI8ci","execution":{"iopub.status.busy":"2022-03-23T13:08:58.267106Z","iopub.execute_input":"2022-03-23T13:08:58.267814Z","iopub.status.idle":"2022-03-23T13:08:58.423797Z","shell.execute_reply.started":"2022-03-23T13:08:58.267777Z","shell.execute_reply":"2022-03-23T13:08:58.423014Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# show the structure\nmodel_1.summary()","metadata":{"id":"QM1u1jy6Lu0Q","outputId":"d9cb47ef-9008-4a55-90c4-e0ba00c04da3","execution":{"iopub.status.busy":"2022-03-23T13:08:58.427714Z","iopub.execute_input":"2022-03-23T13:08:58.427920Z","iopub.status.idle":"2022-03-23T13:08:58.444174Z","shell.execute_reply.started":"2022-03-23T13:08:58.427894Z","shell.execute_reply":"2022-03-23T13:08:58.443486Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# report the total number of parameters in the model\nprint('Total params in model_1:', model_1.count_params())","metadata":{"id":"a3xJ4aTpm613","outputId":"80bfce0b-71bc-438e-83e9-0fcdf7f74516","execution":{"iopub.status.busy":"2022-03-23T13:08:58.446528Z","iopub.execute_input":"2022-03-23T13:08:58.446786Z","iopub.status.idle":"2022-03-23T13:08:58.451943Z","shell.execute_reply.started":"2022-03-23T13:08:58.446753Z","shell.execute_reply":"2022-03-23T13:08:58.451112Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# constructing multiple models with the same type of architecture, \n# increasing the number of filters\nfilters2_1 = 32 \nfilters2_2 = 64 \nfilters2_3 = 256\nmodel_2 = build_basic(filters2_1, filters2_2, filters2_3)\n\nfilters3_1 = 64 \nfilters3_2 = 128 \nfilters3_3 = 256\nmodel_3 = build_basic(filters3_1, filters3_2, filters3_3)\n\nfilters4_1 = 128 \nfilters4_2 = 256 \nfilters4_3 = 512\nmodel_4 = build_basic(filters4_1, filters4_2, filters4_3)","metadata":{"id":"_yH1Wv11HOjw","execution":{"iopub.status.busy":"2022-03-23T13:08:58.453266Z","iopub.execute_input":"2022-03-23T13:08:58.453618Z","iopub.status.idle":"2022-03-23T13:08:58.735887Z","shell.execute_reply.started":"2022-03-23T13:08:58.453525Z","shell.execute_reply":"2022-03-23T13:08:58.735137Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# report the total number of parameters in the model\nprint('Total params in model_2:', model_2.count_params())\nprint('Total params in model_3:', model_3.count_params())\nprint('Total params in model_4:', model_4.count_params())","metadata":{"id":"gIZdmHhLKpbE","outputId":"f61a33b3-ebae-43fe-c300-94f284654e05","execution":{"iopub.status.busy":"2022-03-23T13:08:58.737299Z","iopub.execute_input":"2022-03-23T13:08:58.737541Z","iopub.status.idle":"2022-03-23T13:08:58.747784Z","shell.execute_reply.started":"2022-03-23T13:08:58.737507Z","shell.execute_reply":"2022-03-23T13:08:58.747059Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# generate a plot showing the relationship\nfilter1_sum = filters1_1 + filters1_2 + filters1_3\nfilter2_sum = filters2_1 + filters2_2 + filters2_3\nfilter3_sum = filters3_1 + filters3_2 + filters3_3\nfilter4_sum = filters4_1 + filters4_2 + filters4_3\n\nFilters_sum = [filter1_sum, filter2_sum, filter3_sum, filter4_sum]\nParams = [model_1.count_params(), model_2.count_params(), model_3.count_params(), model_4.count_params()]\n\nplt.title('Relationship')\nplt.xlabel('Filters')\nplt.ylabel('Params')\nplt.plot(Filters_sum, Params, color='r')\nplt.show()","metadata":{"id":"0XIVp3o2Llvz","outputId":"7ec8ca34-d211-4398-b4d0-1c78c505dcf6","execution":{"iopub.status.busy":"2022-03-23T13:08:58.749245Z","iopub.execute_input":"2022-03-23T13:08:58.750191Z","iopub.status.idle":"2022-03-23T13:08:59.165666Z","shell.execute_reply.started":"2022-03-23T13:08:58.750149Z","shell.execute_reply":"2022-03-23T13:08:59.164890Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### 1.2\n\n**1.2 Choosing a Model, Training and Evaluating It. [20pts total]**\n\n\n **[15pts]** Take your model from above and train it. You can choose to train your model for as long as you'd like, but you should aim for at least 10 epochs.  Your validation accuracy should exceed 70%. Training for 10 epochs on a CPU should take about 30-60 minutes.\n \n **[5pts]** Plot the loss and accuracy (both train and test) for your chosen architecture.","metadata":{"id":"_BGi-pD3XKX5"}},{"cell_type":"code","source":"# your code here\n\n# train the model\nmodel_1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","metadata":{"id":"lINkjX8hXKX6","execution":{"iopub.status.busy":"2022-03-23T13:08:59.167170Z","iopub.execute_input":"2022-03-23T13:08:59.167408Z","iopub.status.idle":"2022-03-23T13:08:59.181345Z","shell.execute_reply.started":"2022-03-23T13:08:59.167375Z","shell.execute_reply":"2022-03-23T13:08:59.180649Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"history = model_1.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))","metadata":{"id":"benhNLRsM66i","outputId":"d175d2f8-f9b0-446f-8f26-2b11cab3f19c","execution":{"iopub.status.busy":"2022-03-23T13:08:59.182966Z","iopub.execute_input":"2022-03-23T13:08:59.183385Z","iopub.status.idle":"2022-03-23T13:10:22.363892Z","shell.execute_reply.started":"2022-03-23T13:08:59.183346Z","shell.execute_reply":"2022-03-23T13:10:22.363118Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# plot the loss and accuracy\n\n# h = history.history.copy()\n# del h['accuracy']\n# del h['val_accuracy']\n# pd.DataFrame(h).plot()\n\n# h = history.history.copy()\n# del h['loss']\n# del h['val_loss']\n# pd.DataFrame(h).plot()\n\nfig, axs = plt.subplots(1, 2, figsize=(15,5)) \n\naxs[0].plot(history.history['loss']) \naxs[0].plot(history.history['val_loss']) \naxs[0].set_title('Loss')\naxs[0].set_ylabel('Loss') \naxs[0].set_xlabel('Epoch')\naxs[0].legend(['train', 'val'])\n\naxs[1].plot(history.history['accuracy']) \naxs[1].plot(history.history['val_accuracy']) \naxs[1].set_title('Accuracy')\naxs[1].set_ylabel('Accuracy') \naxs[1].set_xlabel('Epoch')\naxs[1].legend(['train', 'val'])\n\nplt.show()\n","metadata":{"id":"0LU1NbqtSvpL","outputId":"d0321020-e381-4b77-a37e-f835c365751c","execution":{"iopub.status.busy":"2022-03-23T13:10:22.365598Z","iopub.execute_input":"2022-03-23T13:10:22.365879Z","iopub.status.idle":"2022-03-23T13:10:22.658612Z","shell.execute_reply.started":"2022-03-23T13:10:22.365843Z","shell.execute_reply":"2022-03-23T13:10:22.657930Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# evaluate the model\nmodel_1_loss, model_1_accuracy = model_1.evaluate(X_test, y_test)\nprint('test loss:', model_1_loss)\nprint('test accuracy:', model_1_accuracy)","metadata":{"id":"K_oWCpB4njv0","outputId":"ec7dcd00-3b83-4c2f-9479-11574f79da22","execution":{"iopub.status.busy":"2022-03-23T13:10:22.659837Z","iopub.execute_input":"2022-03-23T13:10:22.660197Z","iopub.status.idle":"2022-03-23T13:10:23.550592Z","shell.execute_reply.started":"2022-03-23T13:10:22.660159Z","shell.execute_reply":"2022-03-23T13:10:23.549861Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### 1.3\n\n\n**1.3 [14pts]** For a given input image from the test set that is correctly classified, use your model and extract 9 feature maps from an intermediate convolutional layer of your choice and plot the images in a 3x3 grid (use `imshow`'s `cmap='gray'` to show the feature maps in black & white).  Make sure to plot (and clearly label) your original input image as well. You may use the provided `get_feature_maps` function and the `cifar10dict` dictionary to convert class index to the correct class name.","metadata":{"id":"YHMhy_VPXKX6"}},{"cell_type":"markdown","source":"**Helper code to generate feature maps**","metadata":{"id":"7DpDTCk3XKX6"}},{"cell_type":"code","source":"def get_feature_maps(model, layer_id, input_image):\n    \"\"\"Returns intermediate output (activation map) from passing an image to the model\n    \n    Parameters:\n        model (tf.keras.Model): Model to examine\n        layer_id (int): Which layer's (from zero) output to return\n        input_image (ndarray): The input image\n    Returns:\n        maps (List[ndarray]): Feature map stack output by the specified layer\n    \"\"\"\n    model_ = Model(inputs=[model.input], outputs=[model.layers[layer_id].output])\n    return model_.predict(np.expand_dims(input_image, axis=0))[0,:,:,:].transpose((2,0,1))","metadata":{"id":"KEBxk45KXKX7","execution":{"iopub.status.busy":"2022-03-23T13:10:23.551775Z","iopub.execute_input":"2022-03-23T13:10:23.552492Z","iopub.status.idle":"2022-03-23T13:10:23.558995Z","shell.execute_reply.started":"2022-03-23T13:10:23.552451Z","shell.execute_reply":"2022-03-23T13:10:23.558203Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"**A dictionary to turn class index into class labels for CIFAR-10**","metadata":{"id":"4XVeFnCQXKX7"}},{"cell_type":"code","source":"cifar10dict = {0 : 'airplane', 1 : 'automobile', 2 : 'bird', 3 : 'cat', 4 : 'deer', 5 : 'dog', 6 : 'frog', 7 : 'horse', 8 : 'ship', 9 : 'truck'}","metadata":{"id":"n-Nw5WfDXKX7","execution":{"iopub.status.busy":"2022-03-23T13:10:23.560076Z","iopub.execute_input":"2022-03-23T13:10:23.560895Z","iopub.status.idle":"2022-03-23T13:10:23.571843Z","shell.execute_reply.started":"2022-03-23T13:10:23.560856Z","shell.execute_reply":"2022-03-23T13:10:23.571143Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# your code here\n\n# choose 9 images from X_test randomly\nimport random\n\nchosen_img = []\nchosen_index = []\n\nfor i in range(9):    \n    chosen_index.append(random.randint(0, X_test.shape[0]-1))\n    chosen_img.append(X_test[chosen_index[i]])","metadata":{"id":"QdiMcOGqeose","execution":{"iopub.status.busy":"2022-03-23T13:10:23.582318Z","iopub.execute_input":"2022-03-23T13:10:23.582616Z","iopub.status.idle":"2022-03-23T13:10:23.590257Z","shell.execute_reply.started":"2022-03-23T13:10:23.582580Z","shell.execute_reply":"2022-03-23T13:10:23.589500Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# get the labels\ny_true = np.argmax(y_test)\nlabels = y_test.reshape(1, -1)[0]","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:10:23.591420Z","iopub.execute_input":"2022-03-23T13:10:23.592112Z","iopub.status.idle":"2022-03-23T13:10:23.599249Z","shell.execute_reply.started":"2022-03-23T13:10:23.592083Z","shell.execute_reply":"2022-03-23T13:10:23.598583Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# show the chosen images\nfig, axes = plt.subplots(3, 3, figsize=(6, 6))\naxes = axes.ravel()\n\nfor i in range(9):\n    axes[i].axis('off')\n    plt.subplots_adjust(wspace=1)\n    axes[i].set_title(\"Label: %s\" % (cifar10dict[labels[chosen_index[i]]]))\n    axes[i].imshow(chosen_img[i])","metadata":{"id":"XucjpJbUfNio","outputId":"30e64973-591f-4d11-ea3c-b01888a21112","execution":{"iopub.status.busy":"2022-03-23T13:12:46.792811Z","iopub.execute_input":"2022-03-23T13:12:46.793288Z","iopub.status.idle":"2022-03-23T13:12:47.164966Z","shell.execute_reply.started":"2022-03-23T13:12:46.793249Z","shell.execute_reply":"2022-03-23T13:12:47.164203Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# get 9 feature maps from the intermediate convolutional layer\nf_map = []\nfor i in range(9):\n    f_map.append(get_feature_maps(model_1, 10, chosen_img[i])[0])","metadata":{"id":"74clKSVwhBhY","execution":{"iopub.status.busy":"2022-03-23T13:10:23.973942Z","iopub.execute_input":"2022-03-23T13:10:23.974168Z","iopub.status.idle":"2022-03-23T13:10:24.885943Z","shell.execute_reply.started":"2022-03-23T13:10:23.974136Z","shell.execute_reply":"2022-03-23T13:10:24.885017Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# show the features maps\nfig, axes = plt.subplots(3, 3, figsize=(6, 6))\naxes = axes.ravel()\n\nfor i in range(9):\n    axes[i].axis('off')\n    plt.subplots_adjust(wspace=1)\n    axes[i].set_title(\"Label: %s\" % (cifar10dict[labels[chosen_index[i]]]))\n    axes[i].imshow(f_map[i], cmap='gray')","metadata":{"id":"TSoQFPEXoOom","outputId":"deb8d393-ebf2-45a4-dba2-d19832fc1b03","execution":{"iopub.status.busy":"2022-03-23T13:13:06.556274Z","iopub.execute_input":"2022-03-23T13:13:06.556534Z","iopub.status.idle":"2022-03-23T13:13:06.910862Z","shell.execute_reply.started":"2022-03-23T13:13:06.556504Z","shell.execute_reply":"2022-03-23T13:13:06.910132Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# additional work, which has nothing to do with scoring ...\n\n# get the predictions\npred = model_1.predict(X_test)\ny_pred = np.argmax(pred, axis=1) ","metadata":{"id":"bWwVYSfzXKX8","execution":{"iopub.status.busy":"2022-03-23T13:10:25.243817Z","iopub.execute_input":"2022-03-23T13:10:25.244376Z","iopub.status.idle":"2022-03-23T13:10:26.050364Z","shell.execute_reply.started":"2022-03-23T13:10:25.244335Z","shell.execute_reply":"2022-03-23T13:10:26.049620Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# check the shape of prediction\ny_pred","metadata":{"id":"7s8vt1K_UvX8","outputId":"2217d132-4200-423d-bf18-5087e1691f3d","execution":{"iopub.status.busy":"2022-03-23T13:10:26.051818Z","iopub.execute_input":"2022-03-23T13:10:26.052051Z","iopub.status.idle":"2022-03-23T13:10:26.056996Z","shell.execute_reply.started":"2022-03-23T13:10:26.052019Z","shell.execute_reply":"2022-03-23T13:10:26.056336Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# get the labels\ny_true = np.argmax(y_test)\nlabels = y_test.reshape(1, -1)[0]","metadata":{"id":"LTZMQNFiSWJJ","execution":{"iopub.status.busy":"2022-03-23T13:10:26.058307Z","iopub.execute_input":"2022-03-23T13:10:26.058773Z","iopub.status.idle":"2022-03-23T13:10:26.066903Z","shell.execute_reply.started":"2022-03-23T13:10:26.058740Z","shell.execute_reply":"2022-03-23T13:10:26.066194Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# check the shape of label\nlabels","metadata":{"id":"qsY6i42nUy7h","outputId":"b88199ee-ba2e-4714-a3c2-b7472df6bdaa","execution":{"iopub.status.busy":"2022-03-23T13:10:26.068194Z","iopub.execute_input":"2022-03-23T13:10:26.068514Z","iopub.status.idle":"2022-03-23T13:10:26.081169Z","shell.execute_reply.started":"2022-03-23T13:10:26.068479Z","shell.execute_reply":"2022-03-23T13:10:26.080429Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# show the predictions\nfig, axes = plt.subplots(3, 3, figsize=(6, 6))\naxes = axes.ravel()\n\nfor i in np.arange(9):\n    axes[i].axis('off')\n    plt.subplots_adjust(wspace=1)\n    axes[i].set_title(\"True: %s \\nPredict: %s\" % (cifar10dict[labels[chosen_index[i]]], cifar10dict[y_pred[chosen_index[i]]]))\n    axes[i].imshow(X_test[chosen_index[i]], cmap='gray')  ","metadata":{"id":"QXFXsE44PWNW","outputId":"e9850383-1f30-4a9a-c407-3fe059271806","execution":{"iopub.status.busy":"2022-03-23T13:16:42.007491Z","iopub.execute_input":"2022-03-23T13:16:42.008225Z","iopub.status.idle":"2022-03-23T13:16:42.637081Z","shell.execute_reply.started":"2022-03-23T13:16:42.008185Z","shell.execute_reply":"2022-03-23T13:16:42.636412Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#EFF8D0\">\n    \n<h2> Part 2: Regression with CNN [50 pts total] </h2>\n\n## Problem Statement\n\nIn this problem we will construct a neural network to predict *how far a face is from being \"upright\"*. \n\nImage orientation estimation with convolutional networks was first implemented in 2015 by Fischer, Dosovitskiy, and Brox in a paper titled [\"Image Orientation Estimation with Convolutional Networks\"](https://lmb.informatik.uni-freiburg.de/Publications/2015/FDB15/image_orientation.pdf), where the authors trained a network to straighten a wide variety of images using the Microsoft COCO dataset. \n\nIn order to have a reasonable training time for a homework, we will be working on a subset of the problem where we just straighten images of faces. To do this, we will be using the [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset of celebrity faces, where we assume that professional photographers have taken level pictures. \n\nThe training will be supervised, with a rotated image (up to $\\pm 60^\\circ$) as an input, and the amount (in degrees) that the image has been rotated as a target. \n    \n</div>","metadata":{"id":"4NtK5QlaXKX8"}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#EFF8D0\">\n    \n<h2> Questions </h2>\n\n### Data preparation [20 points]\n\n**2.1.1[10 points]** **Loading CelebA and Thinking about Datasets**.\n\nRun the cells provided to automatically download the CelebA dataset. It is about 1.3GB, which can take 10-20 minutes to download. This happens only once; in the future when you rerun the cell, it will use the dataset stored on your google drive.\n\n**NOTE**: If you get a `NonMatchingChecksumError`, note that this is a documented issue and is because of multiple server requests. Refer [here](https://github.com/tensorflow/datasets/issues/1482) for more details.\n\nThe creation of the normalization/rotation/resize pipeline has been done for you, resulting in train dataset `train_rot_ds` and test dataset `test_rot_ds`. \n\n[TensorFlow Datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) are objects that behave like Python generators, allowing you to take elements (either input/target tuples or feature dictionaries) until you have gone through the entire dataset. Note how this is different from part 1 where the entire dataset was loaded in as an array. \n\nDatasets also allow you to pipeline transformations to be applied to the elements, resulting in a new transformed Dataset (like `train_rot_ds`). \n\nExplain in less than 150 words why using this approach is advantageous over loading the entire data in one array.\n\n   \n**2.1.2[5 points]** **Taking a look**.\n\nIn a grid of subplots, plot at least 4 rotated images from `train_rot_ds` with the titles being the amount the images have been rotated. The floating point numbers in the titles should have a reasonable number of digits. \n\nHint: one way to get a few image+label tuples from the Dataset is with `train_rot_ds.take(4)`. Check the [TensorFlow Datasets documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for more.\n\n**2.1.3[5 points]** **Conceptual Question**\n\nDropout layers have been shown to work well for regularizing deep neural networks, and can be used for very little computational cost. \n\nWrite in **3-5 sentences** if it is a good idea to use dropout layers? \n\nExplain, being sure to explicitly discuss how a dropout layer works, and what that would mean for our model.\n\n### Building and training your CNN [25 points]\n\n**2.2.1[5 points]** **Compiling your model**.\n\nConstruct a model with multiple Conv layers and any other layers you think would help. Be sure to output `<yourmodelname>.summary()` as always. Feel free to experiment with architectures and number of parameters if you wish to get better performance or better training speed. You certainly don't need more than a few million parameters; we were able to it with substantially fewer. Any working setup is acceptable though.\n\n**2.2.2[10 points]** **Training your model**.\n\nTrain your model using `<yourmodelname>.fit()`. The syntax is a little different when working with Datasets instead of numpy arrays; take a look at the [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) documentation. Be sure to also pass the test data as validation data. When passing `train_rot_ds` to `fit()`, you will find it useful to use pipelines to [batch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) the data. You can also experiment with [prefetching](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) batches/elements from the dataset, which may allow you to speed up iterations by a few percent. Finally, while dry-running and prototyping your model, you may find it useful to [take](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take) a subset of the data to speed up experimentation. Your final model should be trained on all the available training data though. You should achieve a validation loss of less than 9, corresponding to $\\pm 3^\\circ$ accuracy in predicting the rotations on the test set. This can be achieved in just 2-3 epochs, though you are free to train as long as you want.\n\n**2.2.3[10 points]** **Evaluating your model**.\n\nCreate a subplots grid with 4 rows and 3 columns. Each row will be a separate image from the test set (of your choice) and each column will consist of: Original Image, Predicted Straightened Image, Target Straightened Image. The title of the latter two should be the predicted rotation and the actual rotation. For example, a row should look something like this:\n\nThis can be achieved using the provided function `rot_resize` to correct for the rotation predicted by your network.\n\n### Further Analysis [5 points]\n\n**2.3.1[5 points]** **Correct an image of your choosing**.\n\nFind an image or image(s) (not from the provided test/training sets), or make your own. You may rotate it yourself up to $\\pm60^\\circ$, or the face can already be naturally rotated. Resize and crop the image to 140px by 120px, load it here, and normalize it to [0.,1.] (you may use the provided `normalize_image` function) and use your network to correct it.\n    \n</div>","metadata":{"id":"kznFyR6EXKX8"}},{"cell_type":"markdown","source":"**2.1.1** **Loading CelebA and Thinking about Datasets**.\n\nRun the cells provided to automatically download and load the CelebA dataset. It is about 1.3GB, and may take some time to download. Please ensure you are running the `2.1.0` version of tensorflow.\n\nThe creation of the normalization/rotation/resize pipeline has been done for you, resulting in train dataset `train_rot_ds` and test dataset `test_rot_ds`. \n\n[TensorFlow Datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) are objects that behave like Python generators, allowing you to take elements (either input/target tuples or feature dictionaries) until you have gone through the entire dataset. Note how this is different from part 1 where the entire dataset was loaded in as an array. \n\nDatasets also allow you to pipeline transformations to be applied to the elements, resulting in a new transformed Dataset (like `train_rot_ds`). ","metadata":{"id":"1PcA1CZQXKX9"}},{"cell_type":"code","source":"#mount your own drive to avoid downloading the data multiple time \n\nfrom google.colab import drive\ndrive.mount('/content/gdrive')","metadata":{"id":"z7aLtwmPXKX9","outputId":"565056a2-e468-49a0-d10c-3ca439264417","execution":{"iopub.status.busy":"2022-03-23T13:10:26.461441Z","iopub.execute_input":"2022-03-23T13:10:26.461906Z","iopub.status.idle":"2022-03-23T13:10:26.784045Z","shell.execute_reply.started":"2022-03-23T13:10:26.461868Z","shell.execute_reply":"2022-03-23T13:10:26.782233Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"#Following creates a directory, downloads the file and unzips it. \n\nif os.path.isdir('gdrive/My Drive/celeb_a/2.0.1'):\n    print('Found dataset' )\nelse:\n    os.makedirs('gdrive/My Drive/celeb_a')\n    !gdown -qq https://drive.google.com/u/2/uc?id=1CdfrT4f87b8ggx02TxsBuMTu0bqSIpNX&export=download\n    !unzip 2.0.1.zip  -d gdrive/My\\ Drive/celeb_a/","metadata":{"id":"aM0ONGGgXKX9","execution":{"iopub.status.busy":"2022-03-23T13:10:26.784882Z","iopub.status.idle":"2022-03-23T13:10:26.785193Z","shell.execute_reply.started":"2022-03-23T13:10:26.785030Z","shell.execute_reply":"2022-03-23T13:10:26.785055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This command will use the celeb_a dataset that you downloaded, and load it into train and test 'tensorflow.Datasets'\n\ntrain_celeb, test_celeb = tfds.load('celeb_a', split=['train', 'test'], shuffle_files=False, data_dir = 'gdrive/My Drive/', download=False)","metadata":{"id":"G_LHk62MXKX-","execution":{"iopub.status.busy":"2022-03-23T13:10:26.786411Z","iopub.status.idle":"2022-03-23T13:10:26.786838Z","shell.execute_reply.started":"2022-03-23T13:10:26.786612Z","shell.execute_reply":"2022-03-23T13:10:26.786634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# You may use the following two functions\ndef normalize_image(img):\n    return tf.cast(img, tf.float32)/255.\n\ndef rot_resize(img, deg):\n    rotimg = ndimage.rotate(img, deg, reshape=False, order=3)\n    rotimg = np.clip(rotimg, 0., 1.)\n    rotimg = tf.image.resize_with_crop_or_pad(rotimg,140,120)\n    return rotimg\n\n################################################################\n# Don't manually invoke these functions; they are for Dataset \n# pipelining that is already done for you.\n################################################################\ndef tf_rot_resize(img, deg):\n    \"\"\"Dataset pipe that rotates an image and resizes it to 140x120\"\"\"\n    rotimg = tfa.image.rotate(img, deg/180.*np.pi, interpolation=\"BILINEAR\")\n    rotimg = tf.image.resize_with_crop_or_pad(rotimg,140,120)\n    return rotimg\n\ndef tf_random_rotate_helper(image):\n    \"\"\"Dataset pipe that normalizes image to [0.,1.] and rotates by a random\n    amount of degrees in [-60.,60.], returning an (input,target) pair consisting\n    of the rotated and resized image and the degrees it has been rotated by.\"\"\"\n    image = normalize_image(image)\n    deg = tf.random.uniform([],-60.,60.)\n    return (tf_rot_resize(image,deg), deg)  # (data, label)\n\ndef tf_random_rotate_image(element):\n    \"\"\"Given an element drawn from the CelebA dataset, this returns a rotated\n    image and the amount it has been rotated by, in degrees.\"\"\"\n    image = element['image']\n    image, label = tf_random_rotate_helper(image)\n    image.set_shape((140,120,3))\n    return image, label\n################################################################","metadata":{"id":"UJXLbueXXKX-","execution":{"iopub.status.busy":"2022-03-23T13:10:26.787836Z","iopub.status.idle":"2022-03-23T13:10:26.788200Z","shell.execute_reply.started":"2022-03-23T13:10:26.787993Z","shell.execute_reply":"2022-03-23T13:10:26.788015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pipeline for creating randomly rotated images with their target labels being \n# the amount they were rotated, in degrees.\ntrain_rot_ds = train_celeb.map(tf_random_rotate_image)\ntest_rot_ds = test_celeb.map(tf_random_rotate_image)","metadata":{"id":"2lpispcWXKX-","execution":{"iopub.status.busy":"2022-03-23T13:10:26.789197Z","iopub.status.idle":"2022-03-23T13:10:26.790032Z","shell.execute_reply.started":"2022-03-23T13:10:26.789788Z","shell.execute_reply":"2022-03-23T13:10:26.789814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Your answer here**","metadata":{"id":"26P7z0ZbXKX_"}},{"cell_type":"markdown","source":"**2.1.2** **Taking a look**.\n\nIn a grid of subplots, plot at least 4 rotated images from `train_rot_ds` with the titles being the amount the images have been rotated. The floating point numbers in the titles should have a reasonable number of digits. \n\nHint: one way to get a few image+label tuples from the Dataset is with `train_rot_ds.take(4)`. Check the [TensorFlow Datasets documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for more.","metadata":{"id":"ITRYWd27XKX_"}},{"cell_type":"code","source":"# your code here","metadata":{"id":"h7lpyQHuXKX_","execution":{"iopub.status.busy":"2022-03-23T13:10:26.791467Z","iopub.status.idle":"2022-03-23T13:10:26.791898Z","shell.execute_reply.started":"2022-03-23T13:10:26.791678Z","shell.execute_reply":"2022-03-23T13:10:26.791701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.1.3** **Conceptual Question**\n\nDropout layers have been shown to work well for regularizing deep neural networks, and can be used for very little computational cost. \n\nWrite in **3-5 sentences** if it is a good idea to use dropout layers? \n\nExplain, being sure to explicitly discuss how a dropout layer works, and what that would mean for our model.","metadata":{"id":"Ijf8GI9sXKX_"}},{"cell_type":"markdown","source":"*Your answer here*","metadata":{"id":"gk-Ak0HuXKX_"}},{"cell_type":"markdown","source":"**2.2.1** **Compiling your model**.\n\nConstruct a model with multiple Conv layers and any other layers you think would help. Be sure to output `<yourmodelname>.summary()` as always. Feel free to experiment with architectures and number of parameters if you wish to get better performance or better training speed. You certainly don't need more than a few million parameters; we were able to it with substantially fewer. Any working setup is acceptable though.","metadata":{"id":"hMeUKn4EXKYA"}},{"cell_type":"code","source":"# your code here","metadata":{"id":"3FvOUAqnXKYA","execution":{"iopub.status.busy":"2022-03-23T13:10:26.794201Z","iopub.status.idle":"2022-03-23T13:10:26.795549Z","shell.execute_reply.started":"2022-03-23T13:10:26.795324Z","shell.execute_reply":"2022-03-23T13:10:26.795347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.2.2** **Training your model**.\n\nTrain your model using `<yourmodelname>.fit()`. The syntax is a little different when working with Datasets instead of numpy arrays; take a look at the [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) documentation. Be sure to also pass the test data as validation data. When passing `train_rot_ds` to `fit()`, you will find it useful to use pipelines to [batch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) the data. You can also experiment with [prefetching](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) batches/elements from the dataset, which may allow you to speed up iterations by a few percent. Finally, while dry-running and prototyping your model, you may find it useful to [take](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take) a subset of the data to speed up experimentation. Your final model should be trained on all the available training data though. You should achieve a validation loss of less than 9, corresponding to $\\pm 3^\\circ$ accuracy in predicting the rotations on the test set. This can be achieved in just 2-3 epochs, though you are free to train as long as you want.","metadata":{"id":"fE5IGB82XKYA"}},{"cell_type":"code","source":"# your code here","metadata":{"id":"AhSCo6H_XKYB","execution":{"iopub.status.busy":"2022-03-23T13:10:26.796689Z","iopub.status.idle":"2022-03-23T13:10:26.797546Z","shell.execute_reply.started":"2022-03-23T13:10:26.797315Z","shell.execute_reply":"2022-03-23T13:10:26.797340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.2.3** **Evaluating your model**.\n\nCreate a subplots grid with 4 rows and 3 columns. Each row will be a separate image from the test set (of your choice) and each column will consist of: Original Image, Predicted Straightened Image, Target Straightened Image. The title of the latter two should be the predicted rotation and the actual rotation. For example, a row should look something like this:\n\nThis can be achieved using the provided function `rot_resize` to correct for the rotation predicted by your network.","metadata":{"id":"yaNYc6-6XKYB"}},{"cell_type":"code","source":"# your code here","metadata":{"id":"hhrYyIKHXKYB","execution":{"iopub.status.busy":"2022-03-23T13:10:26.798702Z","iopub.status.idle":"2022-03-23T13:10:26.799632Z","shell.execute_reply.started":"2022-03-23T13:10:26.799346Z","shell.execute_reply":"2022-03-23T13:10:26.799375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.3.1** **Correct an image of your choosing**.\n\nFind an image or image(s) (not from the provided test/training sets), or make your own. You may rotate it yourself up to $\\pm60^\\circ$, or the face can already be naturally rotated. Resize and crop the image to 140px by 120px, load it here, and normalize it to [0.,1.] (you may use the provided `normalize_image` function) and use your network to correct it.","metadata":{"id":"G1uKyLG3XKYB"}},{"cell_type":"code","source":"# your code here","metadata":{"id":"-JTpv5z_XKYB","execution":{"iopub.status.busy":"2022-03-23T13:10:26.800807Z","iopub.status.idle":"2022-03-23T13:10:26.801637Z","shell.execute_reply.started":"2022-03-23T13:10:26.801384Z","shell.execute_reply":"2022-03-23T13:10:26.801408Z"},"trusted":true},"execution_count":null,"outputs":[]}]}